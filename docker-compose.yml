version: '3'

networks:
  data-science:
    ipam:
      driver: default
      config:
        - subnet: 172.30.10.0/24

services:
  hive:
    image: postgres:11.5
    hostname: hive
    environment:
      POSTGRES_PASSWORD: password
    expose:
      - 5432
    volumes:
      - ./hive/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      data-science:
        ipv4_address: 172.30.10.103
    extra_hosts:
      - "spark-master:172.30.10.101"
      - "spark-worker:172.30.10.102"
      - "hive:172.30.10.103"
      - "livy:172.30.10.104"
      - "zeppelin:172.30.10.100"

  spark-master:
    build: base/
    image: jtroy/spark-base:0.1
    command: bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    hostname: spark-master
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_IDENT_STRING: spark-master
      HADOOP_NODE: namenode
      HIVE_CONFIGURE: yes, please
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_MASTER_HOST: 127.0.0.1
      #      SPARK_LOCAL_IP: 172.30.1.101
      #      SPARK_MASTER_HOST: 172.30.1.101
      SPARK_LOCAL_HOSTNAME: spark-master
    volumes:
      - ${PWD}/spark-master-volume/conf:/conf
      - ${PWD}/spark-master-volume/tmp/data:/tmp/data
      - ${PWD}/spark-master-volume/notebook:/cluster/notebook/
    networks:
      data-science:
        ipv4_address: 172.30.10.101
    extra_hosts:
      - "spark-master:172.30.10.101"
      - "spark-worker:172.30.10.102"
      - "hive:172.30.10.103"
      - "livy:172.30.10.104"
      - "zeppelin:172.30.10.100"
    ports:
      - 8000:8080


      # Spark job Web UI: increments for each successive job
      - 4040:4040
      - 4041:4041
      - 4042:4042

      # Spark History server
      - 18080:18080
      # YARN UI
      - 8388:8088
      # Hadoop namenode UI
      - 9870:9870

      # Hive JDBC
      - 10000:10000
  spark-worker:
    build: base/
    image: jtroy/spark-base:0.1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    hostname: spark-worker
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_CONF_DIR: /conf
      SPARK_LOCAL_HOSTNAME: spark-worker
      SPARK_IDENT_STRING: spark-worker
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_IP: 172.30.10.102
      SPARK_MASTER_HOST: 172.30.10.101
      HADOOP_NODE: datanode

    depends_on:
      - hive
    volumes:
      - ${PWD}/spark-worker-volume/conf:/conf
      - ${PWD}/spark-worker-volume/tmp/data:/tmp/data
    networks:
      data-science:
        ipv4_address: 172.30.10.102
    extra_hosts:
      - "spark-master:172.30.10.101"
      - "spark-worker:172.30.10.102"
      - "hive:172.30.10.103"
      - "livy:172.30.10.104"
      - "zeppelin:172.30.10.100"
    expose:
      - 4040
    ports:
      - "8081-8100:8081-8100"

  livy:
    build: livy/
    image: livy:0.7.0-hadoop-3.0.0-spark-2.4.1
    hostname: livy
    depends_on:
      - spark-master
      - spark-worker
    volumes:
      - ${PWD}/livy_batches:/livy_batches
      - ${PWD}/data:/data

    environment:
      - SPARK_MASTER="spark://spark-master:7077"
      - LOCAL_DIR_WHITELIST=/data/batches/
      - ENABLE_HIVE_CONTEXT=false
    expose:
      - 8998
    ports:
      - 8998:8998
    networks:
      data-science:
        ipv4_address: 172.30.10.104
    extra_hosts:
      - "spark-master:172.30.10.101"
      - "spark-worker:172.30.10.102"
      - "hive:172.30.10.103"
      - "livy:172.30.10.104"
      - "zeppelin:172.30.10.100"

  zeppelin:
    build: zeppelin/
    image: zeppelin:0.8.2-hadoop-3.0.0-spark-2.4.1
    hostname: zeepelin
    expose:
      - 4040
      - 8890
    ports:
      - 9080:8080
      - 9443:8443

      - 8890:8890
    networks:
      data-science:
        ipv4_address: 172.30.10.100
    volumes:
      - ${PWD}/spark-master-volume/opt/zeppelin/cluster-logs:/opt/zeppelin/logs
      - ${PWD}/spark-master-volume/notebook:/cluster/notebook/
    environment:
      MASTER: "spark://spark-master:7077"
      SPARK_MASTER: "spark://spark-master:7077"
      SPARK_HOME: /usr/spark
    extra_hosts:
      - "spark-master:172.30.10.101"
      - "spark-worker:172.30.10.102"
      - "hive:172.30.10.103"
      - "livy:172.30.10.104"
      - "zeppelin:172.30.10.100"
    depends_on:
      - spark-master
      - spark-worker
      - livy


